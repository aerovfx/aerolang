#!/usr/bin/env python3
"""
ğŸ¬ Video Subtitle Generator - Optimized for MacBook Pro M2
Tá»‘i Æ°u cho Apple Silicon M2 vá»›i 16GB RAM
"""

import os
import sys
import whisper
import torch
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import gc
from typing import List, Tuple, Optional
import argparse
from tqdm import tqdm
import psutil
import tkinter as tk
from tkinter import filedialog, messagebox

class VideoProcessor:
    def __init__(self, model_size: str = "base", max_workers: int = 2):
        """
        Khá»Ÿi táº¡o processor vá»›i tá»‘i Æ°u cho M2
        
        Args:
            model_size: KÃ­ch thÆ°á»›c model (tiny, base, small, medium, large)
            max_workers: Sá»‘ luá»“ng tá»‘i Ä‘a (khuyáº¿n nghá»‹ 2 cho M2)
        """
        self.max_workers = max_workers
        self.device = self._setup_device()
        self.model = self._load_model(model_size)
        self.memory_monitor = MemoryMonitor()
        
    def _setup_device(self) -> str:
        """Thiáº¿t láº­p device tá»‘i Æ°u cho Apple Silicon"""
        if torch.backends.mps.is_available():
            device = "mps"
            # Tá»‘i Æ°u MPS settings - kiá»ƒm tra tÆ°Æ¡ng thÃ­ch
            try:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                else:
                    print("âš ï¸  MPS empty_cache khÃ´ng kháº£ dá»¥ng, bá» qua")
            except Exception as e:
                print(f"âš ï¸  Lá»—i MPS cache cleanup: {e}")
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
            
        print(f"ğŸ’» Sá»­ dá»¥ng device: {device}")
        return device
    
    def _load_model(self, model_size: str):
        """Load model vá»›i tá»‘i Æ°u bá»™ nhá»›"""
        print(f"ğŸ“¦ Äang load model {model_size}...")
        
        # Tá»‘i Æ°u cho M2 vá»›i fallback mechanism
        if self.device == "mps":
            try:
                # Thá»­ load vá»›i MPS
                model = whisper.load_model(model_size, device=self.device)
                print(f"âœ… Model {model_size} loaded vá»›i MPS")
                
                # Tá»‘i Æ°u memory usage
                if hasattr(model, 'encoder'):
                    model.encoder = model.encoder.to(dtype=torch.float32)
                    
            except Exception as e:
                print(f"âš ï¸  MPS load failed: {e}")
                print("ğŸ”„ Fallback to CPU...")
                self.device = "cpu"
                model = whisper.load_model(model_size, device="cpu")
                print(f"âœ… Model {model_size} loaded vá»›i CPU fallback")
        else:
            model = whisper.load_model(model_size, device=self.device)
            
        print(f"âœ… Model {model_size} Ä‘Ã£ sáºµn sÃ ng trÃªn {self.device}")
        return model
    
    def format_timestamp(self, seconds: float) -> str:
        """Format timestamp cho SRT"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds - int(seconds)) * 1000)
        return f"{hours:02}:{minutes:02}:{secs:02},{millis:03}"
    
    def transcribe_video(self, video_path: str, output_txt_path: str, 
                        output_srt_path: str, target_lang: str = "vi") -> bool:
        """
        Transcribe video vá»›i tá»‘i Æ°u bá»™ nhá»›
        """
        try:
            print(f"ğŸ¬ Äang xá»­ lÃ½: {Path(video_path).name}")
            
            # Kiá»ƒm tra bá»™ nhá»› trÆ°á»›c khi xá»­ lÃ½
            self.memory_monitor.check_memory()
            
            # Transcribe vá»›i tá»‘i Æ°u cho M2 vÃ  fallback mechanism
            try:
                if target_lang == "en":
                    result = self.model.transcribe(
                        video_path, 
                        task="translate",
                        fp16=False,  # Sá»­ dá»¥ng float32 cho MPS
                        verbose=False
                    )
                else:
                    result = self.model.transcribe(
                        video_path, 
                        language=target_lang,
                        fp16=False,
                        verbose=False
                    )
            except Exception as e:
                if "SparseMPS" in str(e) or "MPS" in str(e):
                    print(f"âš ï¸  MPS error: {e}")
                    print("ğŸ”„ Retrying vá»›i CPU...")
                    
                    # Fallback to CPU
                    self.device = "cpu"
                    self.model = self.model.to("cpu")
                    
                    if target_lang == "en":
                        result = self.model.transcribe(
                            video_path, 
                            task="translate",
                            fp16=False,
                            verbose=False
                        )
                    else:
                        result = self.model.transcribe(
                            video_path, 
                            language=target_lang,
                            fp16=False,
                            verbose=False
                        )
                else:
                    raise e
            
            # LÆ°u file TXT
            with open(output_txt_path, "w", encoding="utf-8") as f:
                f.write(result["text"])
            
            # LÆ°u file SRT
            if "segments" in result:
                with open(output_srt_path, "w", encoding="utf-8") as srt_file:
                    for i, seg in enumerate(result["segments"], 1):
                        start = self.format_timestamp(seg["start"])
                        end = self.format_timestamp(seg["end"])
                        text = seg["text"].strip()
                        srt_file.write(f"{i}\n{start} --> {end}\n{text}\n\n")
            
            # Cleanup memory
            if self.device == "mps":
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except Exception as e:
                    print(f"âš ï¸  Lá»—i MPS cache cleanup: {e}")
            elif self.device == "cuda":
                torch.cuda.empty_cache()
            
            gc.collect()
            
            print(f"âœ… HoÃ n thÃ nh: {Path(video_path).name}")
            return True
            
        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ {Path(video_path).name}: {str(e)}")
            return False
    
    def process_video_batch(self, video_files: List[str], folder_path: str, 
                           target_lang: str = "vi") -> None:
        """Xá»­ lÃ½ batch video vá»›i Ä‘a luá»“ng"""
        total_files = len(video_files)
        
        print(f"ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ {total_files} video vá»›i {self.max_workers} luá»“ng")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Táº¡o futures
            futures = []
            for filename in video_files:
                video_path = os.path.join(folder_path, filename)
                base = os.path.splitext(video_path)[0]
                txt_path = base + "_translated.txt"
                srt_path = base + "_translated.srt"
                
                future = executor.submit(
                    self.transcribe_video, 
                    video_path, txt_path, srt_path, target_lang
                )
                futures.append(future)
            
            # Theo dÃµi tiáº¿n Ä‘á»™
            completed = 0
            failed = 0
            
            with tqdm(total=total_files, desc="ğŸ¬ Xá»­ lÃ½ video") as pbar:
                for future in as_completed(futures):
                    try:
                        success = future.result()
                        if success:
                            completed += 1
                        else:
                            failed += 1
                    except Exception as e:
                        print(f"âŒ Lá»—i: {str(e)}")
                        failed += 1
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'Completed': completed,
                        'Failed': failed,
                        'Memory': f"{self.memory_monitor.get_memory_usage():.1f}%"
                    })
        
        print(f"\nğŸ“Š Káº¿t quáº£: {completed} thÃ nh cÃ´ng, {failed} tháº¥t báº¡i")
    
    def process_folder(self, folder_path: str, target_lang: str = "vi") -> None:
        """Xá»­ lÃ½ toÃ n bá»™ thÆ° má»¥c"""
        folder = Path(folder_path)
        if not folder.exists():
            print("âŒ ThÆ° má»¥c khÃ´ng tá»“n táº¡i")
            return
        
        # TÃ¬m video files
        video_extensions = {'.mp4', '.mov', '.mkv', '.avi', '.flv', '.m4v', '.webm'}
        video_files = [
            f.name for f in folder.iterdir()
            if f.is_file() and f.suffix.lower() in video_extensions
        ]
        
        if not video_files:
            print("â— KhÃ´ng tÃ¬m tháº¥y video há»£p lá»‡ trong thÆ° má»¥c")
            return
        
        print(f"ğŸ“ TÃ¬m tháº¥y {len(video_files)} video files")
        
        # Xá»­ lÃ½ batch
        self.process_video_batch(video_files, folder_path, target_lang)


class MemoryMonitor:
    """Monitor bá»™ nhá»› há»‡ thá»‘ng"""
    
    def __init__(self):
        self.warning_threshold = 80  # 80% RAM usage
        self.critical_threshold = 90  # 90% RAM usage
    
    def get_memory_usage(self) -> float:
        """Láº¥y tá»· lá»‡ sá»­ dá»¥ng RAM"""
        return psutil.virtual_memory().percent
    
    def check_memory(self):
        """Kiá»ƒm tra vÃ  cáº£nh bÃ¡o bá»™ nhá»›"""
        usage = self.get_memory_usage()
        if usage > self.critical_threshold:
            print(f"âš ï¸  Cáº¢NH BÃO: RAM usage {usage:.1f}% - QuÃ¡ cao!")
            gc.collect()
            time.sleep(1)
        elif usage > self.warning_threshold:
            print(f"âš ï¸  RAM usage: {usage:.1f}%")


def get_optimal_model_size(ram_gb: int = 16) -> str:
    """Chá»n model size phÃ¹ há»£p vá»›i RAM"""
    if ram_gb >= 32:
        return "large"
    elif ram_gb >= 16:
        return "medium"  # Tá»‘i Æ°u cho M2 16GB
    elif ram_gb >= 8:
        return "small"
    else:
        return "base"


def select_folder_gui():
    """Chá»n thÆ° má»¥c báº±ng GUI dialog"""
    # Táº¡o root window áº©n
    root = tk.Tk()
    root.withdraw()  # áº¨n window chÃ­nh
    
    # Hiá»ƒn thá»‹ dialog chá»n thÆ° má»¥c
    folder_path = filedialog.askdirectory(
        title="ğŸ¬ Chá»n thÆ° má»¥c chá»©a video",
        initialdir=os.path.expanduser("~/Desktop")  # Báº¯t Ä‘áº§u tá»« Desktop
    )
    
    root.destroy()  # ÄÃ³ng window
    return folder_path

def main():
    parser = argparse.ArgumentParser(description="ğŸ¬ Video Subtitle Generator - M2 Optimized")
    parser.add_argument("folder", nargs='?', help="ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a video (tÃ¹y chá»n)")
    parser.add_argument("--lang", "-l", default="vi", help="NgÃ´n ngá»¯ Ä‘áº§u ra (máº·c Ä‘á»‹nh: vi)")
    parser.add_argument("--model", "-m", default="auto", 
                       choices=["tiny", "base", "small", "medium", "large", "auto"],
                       help="KÃ­ch thÆ°á»›c model (máº·c Ä‘á»‹nh: auto)")
    parser.add_argument("--workers", "-w", type=int, default=2,
                       help="Sá»‘ luá»“ng tá»‘i Ä‘a (máº·c Ä‘á»‹nh: 2)")
    parser.add_argument("--gui", "-g", action="store_true", 
                       help="Sá»­ dá»¥ng GUI Ä‘á»ƒ chá»n thÆ° má»¥c")
    
    args = parser.parse_args()
    
    # XÃ¡c Ä‘á»‹nh thÆ° má»¥c
    folder_path = args.folder
    
    # Náº¿u khÃ´ng cÃ³ thÆ° má»¥c Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh hoáº·c cÃ³ flag --gui
    if not folder_path or args.gui:
        print("ğŸ¬ Video Subtitle Generator - MacBook Pro M2 Optimized")
        print("=" * 50)
        
        if not folder_path:
            print("ğŸ“ ChÆ°a chá»n thÆ° má»¥c. Má»Ÿ dialog chá»n thÆ° má»¥c...")
        
        # Hiá»ƒn thá»‹ dialog chá»n thÆ° má»¥c
        folder_path = select_folder_gui()
        
        if not folder_path:
            print("âŒ KhÃ´ng cÃ³ thÆ° má»¥c nÃ o Ä‘Æ°á»£c chá»n. ThoÃ¡t chÆ°Æ¡ng trÃ¬nh.")
            return
        
        print(f"âœ… ÄÃ£ chá»n thÆ° má»¥c: {folder_path}")
    
    # Kiá»ƒm tra thÆ° má»¥c tá»“n táº¡i
    if not os.path.isdir(folder_path):
        print(f"âŒ ThÆ° má»¥c khÃ´ng tá»“n táº¡i: {folder_path}")
        return
    
    # Chá»n model size
    if args.model == "auto":
        model_size = get_optimal_model_size()
        print(f"ğŸ¤– Tá»± Ä‘á»™ng chá»n model: {model_size}")
    else:
        model_size = args.model
    
    # Khá»Ÿi táº¡o processor
    processor = VideoProcessor(model_size=model_size, max_workers=args.workers)
    
    # Xá»­ lÃ½ thÆ° má»¥c
    processor.process_folder(folder_path, args.lang)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
    except Exception as e:
        print(f"âŒ Lá»—i: {str(e)}")
        sys.exit(1)
