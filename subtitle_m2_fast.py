#!/usr/bin/env python3
"""
ğŸ¬ Fast Video Subtitle Generator - MacBook Pro M2
PhiÃªn báº£n nhanh vá»›i progress tracking tá»‘t hÆ¡n
"""

import os
import sys
import argparse
import whisper
import torch
import gc
import time
import psutil
from pathlib import Path
from tqdm import tqdm

class FastM2SubtitleGenerator:
    def __init__(self, model_size="tiny", max_workers=1):
        """
        Khá»Ÿi táº¡o subtitle generator nhanh cho M2
        
        Args:
            model_size: KÃ­ch thÆ°á»›c model (tiny, base, small, medium, large)
            max_workers: Sá»‘ luá»“ng xá»­ lÃ½ (khuyáº¿n nghá»‹ 1 cho nhanh)
        """
        self.model_size = model_size
        self.max_workers = max_workers
        self.model = None
        self.device = None
        
        # Setup device
        self._setup_device()
        
        print(f"ğŸ¬ Fast M2 Subtitle Generator - Model: {model_size}")
        print(f"ğŸ’» Device: {self.device}")
        print(f"ğŸ”§ Workers: {self.max_workers}")
    
    def _setup_device(self):
        """Setup device tá»‘i Æ°u cho M2"""
        try:
            if torch.backends.mps.is_available():
                self.device = "mps"
                print("âœ… Sá»­ dá»¥ng MPS (Apple Silicon GPU)")
                
                # Tá»‘i Æ°u MPS settings
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except Exception as e:
                    print(f"âš ï¸ MPS cache cleanup warning: {e}")
                
            elif torch.cuda.is_available():
                self.device = "cuda"
                print("âœ… Sá»­ dá»¥ng CUDA")
                
            else:
                self.device = "cpu"
                print("âœ… Sá»­ dá»¥ng CPU")
                
        except Exception as e:
            print(f"âš ï¸ Device setup error: {e}")
            self.device = "cpu"
            print("ğŸ”„ Fallback to CPU")
    
    def load_model(self):
        """Load Whisper model vá»›i tá»‘i Æ°u M2"""
        print(f"ğŸ“¦ Loading model {self.model_size}...")
        
        try:
            # Load model vá»›i device tá»‘i Æ°u
            self.model = whisper.load_model(self.model_size, device=self.device)
            
            # Tá»‘i Æ°u model cho M2
            if self.device == "mps":
                # Sá»­ dá»¥ng float32 cho MPS
                self.model = self.model.float()
                
                # Clear cache
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except Exception as e:
                    print(f"âš ï¸ MPS cache cleanup warning: {e}")
            
            print(f"âœ… Model {self.model_size} loaded successfully")
            return True
            
        except Exception as e:
            print(f"âŒ Model loading error: {e}")
            
            # Fallback to CPU náº¿u MPS lá»—i
            if self.device == "mps":
                print("ğŸ”„ Retrying with CPU...")
                self.device = "cpu"
                self.model = whisper.load_model(self.model_size, device="cpu")
                print("âœ… Model loaded on CPU")
                return True
            
            return False
    
    def format_timestamp(self, seconds):
        """Format timestamp cho SRT"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds - int(seconds)) * 1000)
        return f"{hours:02}:{minutes:02}:{secs:02},{millis:03}"
    
    def transcribe_video(self, video_path, output_txt_path, output_srt_path, target_lang="vi"):
        """
        Transcribe video vÃ  táº¡o file .srt vÃ  .txt
        
        Args:
            video_path: ÄÆ°á»ng dáº«n video
            output_txt_path: ÄÆ°á»ng dáº«n file .txt output
            output_srt_path: ÄÆ°á»ng dáº«n file .srt output
            target_lang: NgÃ´n ngá»¯ Ä‘Ã­ch (vi, en, auto)
        """
        try:
            video_name = Path(video_path).name
            
            # Transcribe vá»›i tá»‘i Æ°u M2
            start_time = time.time()
            
            try:
                if target_lang == "en":
                    result = self.model.transcribe(
                        video_path,
                        task="translate",
                        fp16=False,  # Sá»­ dá»¥ng float32 cho MPS
                        verbose=False
                    )
                else:
                    result = self.model.transcribe(
                        video_path,
                        language=target_lang,
                        fp16=False,
                        verbose=False
                    )
                    
            except Exception as e:
                # Fallback mechanism cho MPS errors
                if "SparseMPS" in str(e) or "MPS" in str(e):
                    print(f"âš ï¸ MPS error: {e}")
                    print("ğŸ”„ Retrying with CPU...")
                    
                    # Switch to CPU
                    self.device = "cpu"
                    self.model = self.model.to("cpu")
                    
                    if target_lang == "en":
                        result = self.model.transcribe(
                            video_path,
                            task="translate",
                            fp16=False,
                            verbose=False
                        )
                    else:
                        result = self.model.transcribe(
                            video_path,
                            language=target_lang,
                            fp16=False,
                            verbose=False
                        )
                else:
                    raise e
            
            transcribe_time = time.time() - start_time
            
            # Táº¡o file TXT
            with open(output_txt_path, "w", encoding="utf-8") as f:
                f.write(result["text"])
            
            # Táº¡o file SRT
            if "segments" in result:
                with open(output_srt_path, "w", encoding="utf-8") as srt_file:
                    for i, seg in enumerate(result["segments"], 1):
                        start = self.format_timestamp(seg["start"])
                        end = self.format_timestamp(seg["end"])
                        text = seg["text"].strip()
                        srt_file.write(f"{i}\n{start} --> {end}\n{text}\n\n")
            
            # Memory cleanup
            self._cleanup_memory()
            
            return True, transcribe_time
            
        except Exception as e:
            return False, 0
    
    def _cleanup_memory(self):
        """Cleanup memory tá»‘i Æ°u cho M2"""
        try:
            if self.device == "mps":
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                except Exception as e:
                    pass
            elif self.device == "cuda":
                torch.cuda.empty_cache()
            
            gc.collect()
            
        except Exception as e:
            pass
    
    def process_folder(self, folder_path, target_lang="vi"):
        """
        Xá»­ lÃ½ táº¥t cáº£ video trong thÆ° má»¥c vá»›i progress bar
        
        Args:
            folder_path: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a video
            target_lang: NgÃ´n ngá»¯ Ä‘Ã­ch
        """
        print(f"ğŸ“ Processing folder: {folder_path}")
        
        # TÃ¬m video files
        video_extensions = {'.mp4', '.mov', '.mkv', '.avi', '.flv', '.m4v', '.webm'}
        video_files = []
        
        for file in os.listdir(folder_path):
            if Path(file).suffix.lower() in video_extensions:
                video_files.append(file)
        
        if not video_files:
            print("âŒ No video files found")
            return False
        
        print(f"ğŸ“¹ Found {len(video_files)} video files")
        
        # Load model náº¿u chÆ°a load
        if not self.model:
            if not self.load_model():
                return False
        
        # Xá»­ lÃ½ tá»«ng video vá»›i progress bar
        success_count = 0
        total_time = 0
        
        with tqdm(total=len(video_files), desc="ğŸ¬ Processing videos", unit="video") as pbar:
            for filename in video_files:
                video_path = os.path.join(folder_path, filename)
                base = os.path.splitext(video_path)[0]
                txt_path = base + "_translated.txt"
                srt_path = base + "_translated.srt"
                
                # Update progress bar description
                pbar.set_description(f"ğŸ¬ Processing {filename}")
                
                success, transcribe_time = self.transcribe_video(video_path, txt_path, srt_path, target_lang)
                
                if success:
                    success_count += 1
                    total_time += transcribe_time
                    pbar.set_postfix({
                        'Success': f"{success_count}/{len(video_files)}",
                        'Time': f"{transcribe_time:.1f}s"
                    })
                else:
                    pbar.set_postfix({
                        'Error': filename,
                        'Success': f"{success_count}/{len(video_files)}"
                    })
                
                pbar.update(1)
        
        print(f"\nğŸ‰ Processing completed!")
        print(f"âœ… Success: {success_count}/{len(video_files)}")
        print(f"â±ï¸ Total time: {total_time:.2f} seconds")
        print(f"ğŸ“Š Average time per video: {total_time/len(video_files):.2f} seconds")
        
        return success_count > 0

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="ğŸ¬ Fast M2 Subtitle Generator")
    
    parser.add_argument("folder", help="ThÆ° má»¥c chá»©a video files")
    parser.add_argument("--lang", default="vi", choices=["vi", "en", "auto"], 
                       help="NgÃ´n ngá»¯ Ä‘Ã­ch (default: vi)")
    parser.add_argument("--model", default="tiny", 
                       choices=["tiny", "base", "small", "medium", "large"],
                       help="KÃ­ch thÆ°á»›c model (default: tiny for speed)")
    parser.add_argument("--workers", type=int, default=1,
                       help="Sá»‘ luá»“ng xá»­ lÃ½ (default: 1 for speed)")
    parser.add_argument("--verbose", action="store_true",
                       help="Hiá»ƒn thá»‹ thÃ´ng tin chi tiáº¿t")
    
    args = parser.parse_args()
    
    # Validate folder
    if not os.path.isdir(args.folder):
        print(f"âŒ Folder not found: {args.folder}")
        sys.exit(1)
    
    print("ğŸ¬ Fast Video Subtitle Generator - MacBook Pro M2")
    print("=" * 60)
    print(f"ğŸ“ Folder: {args.folder}")
    print(f"ğŸŒ Language: {args.lang}")
    print(f"ğŸ“¦ Model: {args.model}")
    print(f"ğŸ”§ Workers: {args.workers}")
    print("=" * 60)
    
    # Create generator
    generator = FastM2SubtitleGenerator(
        model_size=args.model,
        max_workers=args.workers
    )
    
    # Process folder
    start_time = time.time()
    success = generator.process_folder(args.folder, args.lang)
    total_time = time.time() - start_time
    
    print("\n" + "=" * 60)
    if success:
        print(f"ğŸ‰ Processing completed successfully!")
        print(f"â±ï¸ Total time: {total_time:.2f} seconds")
    else:
        print("âŒ Processing failed")
        sys.exit(1)

if __name__ == "__main__":
    main()


